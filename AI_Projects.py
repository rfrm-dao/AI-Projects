import requests
import pandas as pd
import time
import sys
import os

# --- CONFIGURATION ---
# 1. FIX: Read the value into a more descriptive local variable name
CG_API_KEY_VALUE = os.environ.get("CG_API_KEY") # Reads directly from GitHub Secret

# The base URL for the CoinGecko Pro API
API_BASE_URL = 'https://pro-api.coingecko.com/api/v3'
CATEGORY_ID = 'artificial-intelligence'
OUTPUT_FILENAME = 'AI_Projects.csv'

# API headers for authentication
HEADERS = {
Â  Â  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',
Â  Â  # 2. FIX: Use the new variable name in the header
Â  Â  'x-cg-pro-api-key': CG_API_KEY_VALUE, 
}
# ---------------------

def fetch_coin_details(coin_id):
Â  Â  """Fetches full coin details, including the official X (Twitter) handle, using the API."""
Â  Â  detail_url = f'{API_BASE_URL}/coins/{coin_id}'
Â  Â Â 
Â  Â  time.sleep(0.3)Â 
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  response = requests.get(detail_url, headers=HEADERS, timeout=15)
Â  Â  Â  Â  response.raise_for_status()
Â  Â  Â  Â  details = response.json()

Â  Â  Â  Â  twitter_handle = details.get('links', {}).get('twitter_screen_name', None)
Â  Â  Â  Â Â 
Â  Â  Â  Â  return f'@{twitter_handle}' if twitter_handle else 'N/A'
Â  Â  Â  Â Â 
Â  Â  except requests.exceptions.RequestException as e:
Â  Â  Â  Â  print(f"Â  [Error] Failed to fetch details for {coin_id} via API: {e}")
Â  Â  Â  Â  return 'FETCH_ERROR'

def main_scraper():
    # 3. FIX: Check if the key is actually missing (None or empty string)
    if not CG_API_KEY_VALUE:
        print("âŒ ERROR: CG_API_KEY environment variable is missing or empty. Please check your GitHub Secrets.")
        sys.exit(1)
Â  Â  Â  Â Â 
Â  Â  print("--- ğŸš€ Starting CoinGecko AI Social Data Scraper (PAID API) ---")
Â  Â Â 
Â  Â  # 1. Get List of Coins in the Category (using Pro API markets endpoint)
Â  Â  api_markets_url = f'{API_BASE_URL}/coins/markets'
Â  Â  api_params = {
Â  Â  Â  Â  'vs_currency': 'usd',
Â  Â  Â  Â  'category': CATEGORY_ID,
Â  Â  Â  Â  'per_page': 250,
Â  Â  Â  Â  'page': 1,
Â  Â  Â  Â  'order': 'market_cap_desc'
Â  Â  }

Â  Â  try:
Â  Â  Â  Â  print("1/2: Fetching list of all project IDs in the category...")
Â  Â  Â  Â  response = requests.get(api_markets_url, headers=HEADERS, params=api_params, timeout=20)
Â  Â  Â  Â  response.raise_for_status() # This is where the 401 error originates if the key is bad/missing
Â  Â  Â  Â  data = response.json()
Â  Â  Â  Â Â 
Â  Â  Â  Â  if not data:
Â  Â  Â  Â  Â  Â  print("âŒ API returned empty data list.")
Â  Â  Â  Â  Â  Â  return

Â  Â  Â  Â  projects = []
Â  Â  Â  Â  for item in data:
Â  Â  Â  Â  Â  Â  projects.append({
Â  Â  Â  Â  Â  Â  Â  Â  'ID': item.get('id'),
Â  Â  Â  Â  Â  Â  Â  Â  'Project Name': item.get('name'),
Â  Â  Â  Â  Â  Â  Â  Â  'Ticker': item.get('symbol').upper(),
Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  print(f"âœ… Found {len(projects)} projects.")

Â  Â  except requests.exceptions.RequestException as e:
Â  Â  Â  Â  print(f"âŒ Critical API Error while fetching market list: {e}")
Â  Â  Â  Â  sys.exit(1)
Â  Â Â 
Â  Â  # 2. Iterate and fetch details (X username) for each project (API calls)
Â  Â  final_data = []
Â  Â  print("2/2: Fetching X username for each project...")
Â  Â Â 
Â  Â  for i, project in enumerate(projects):
Â  Â  Â  Â Â 
Â  Â  Â  Â  progress = f"[{i+1}/{len(projects)}]"
Â  Â  Â  Â  print(f"{progress} Processing: {project['Project Name']} ({project['Ticker']})", end='\r', flush=True)
Â  Â  Â  Â Â 
Â  Â  Â  Â  x_username = fetch_coin_details(project['ID'])
Â  Â  Â  Â Â 
Â  Â  Â  Â  final_data.append({
Â  Â  Â  Â  Â  Â  'Project Name': project['Project Name'],
Â  Â  Â  Â  Â  Â  'Ticker': project['Ticker'],
Â  Â  Â  Â  Â  Â  'X Username': x_username
Â  Â  Â  Â  })
Â  Â Â 
Â  Â  print("\nâœ… Detail fetching complete.")
Â  Â Â 
Â  Â  # 3. Save to CSV
Â  Â  df = pd.DataFrame(final_data)
Â  Â Â 
Â  Â  df.to_csv(OUTPUT_FILENAME, index=False)
Â  Â  print(f"\nğŸ‰ Success! Data saved to **{OUTPUT_FILENAME}**")


if __name__ == "__main__":
Â  Â  main_scraper()
